{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "74df2c7e",
   "metadata": {},
   "source": [
    "#### **Transfer Learning**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "d8b0aadf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "ef0417fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "57102c68",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.transforms import transforms\n",
    "from torchvision.models import vgg16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "e1435506",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_transform = transforms.Compose([\n",
    "    transforms.Resize(256),\n",
    "    transforms.CenterCrop(224),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "9df0dc1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "c13baf95",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, features, labels, transform):\n",
    "        self.features = features\n",
    "        self.labels = labels\n",
    "        self.transform = transform\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.features) \n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        # Resize to (28,28)\n",
    "        image_path = self.features[index]\n",
    "        \n",
    "        image = Image.open(image_path).convert('RGB')\n",
    "        \n",
    "        # Change datatype to uint8\n",
    "        # image = image.astype(np.uint8)\n",
    "        \n",
    "        # Change b&w to color, 1 -> 3 channels\n",
    "        # image = np.stack([image]*3, axis=-1)\n",
    "        \n",
    "        # Convert array to PIL image\n",
    "        # image = Image.fromarray(image)\n",
    "        \n",
    "        # Apply transformation\n",
    "        image = self.transform(image)\n",
    "        \n",
    "        return image, torch.tensor(self.labels[index], dtype=torch.float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "37bbd0e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "6cfa83e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = datasets.ImageFolder(root='data/cat_vs_dog/training_set')\n",
    "test_dataset = datasets.ImageFolder(root='data/cat_vs_dog/test_set')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "7f8e720a",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_features = [path for path, _ in train_dataset.samples]\n",
    "train_labels = [label for _, label in train_dataset.samples]\n",
    "\n",
    "test_features = [path for path, _ in test_dataset.samples]\n",
    "test_labels = [label for _, label in test_dataset.samples]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "f8589ed5",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = CustomDataset(train_features, train_labels, input_transform)\n",
    "test_dataset = CustomDataset(test_features, test_labels, input_transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "1f72209c",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True, pin_memory=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False, pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "32a77ca0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Ojas\\.vscode\\Repos\\Deep-Learning\\venv\\Lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Ojas\\.vscode\\Repos\\Deep-Learning\\venv\\Lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=VGG16_Weights.IMAGENET1K_V1`. You can also use `weights=VGG16_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "model = vgg16(pretrained=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "b05b9514",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VGG(\n",
       "  (features): Sequential(\n",
       "    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (1): ReLU(inplace=True)\n",
       "    (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (3): ReLU(inplace=True)\n",
       "    (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (5): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (6): ReLU(inplace=True)\n",
       "    (7): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (8): ReLU(inplace=True)\n",
       "    (9): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (10): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (11): ReLU(inplace=True)\n",
       "    (12): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (13): ReLU(inplace=True)\n",
       "    (14): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (15): ReLU(inplace=True)\n",
       "    (16): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (17): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (18): ReLU(inplace=True)\n",
       "    (19): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (20): ReLU(inplace=True)\n",
       "    (21): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (22): ReLU(inplace=True)\n",
       "    (23): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (24): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (25): ReLU(inplace=True)\n",
       "    (26): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (27): ReLU(inplace=True)\n",
       "    (28): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (29): ReLU(inplace=True)\n",
       "    (30): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  )\n",
       "  (avgpool): AdaptiveAvgPool2d(output_size=(7, 7))\n",
       "  (classifier): Sequential(\n",
       "    (0): Linear(in_features=25088, out_features=4096, bias=True)\n",
       "    (1): ReLU(inplace=True)\n",
       "    (2): Dropout(p=0.5, inplace=False)\n",
       "    (3): Linear(in_features=4096, out_features=4096, bias=True)\n",
       "    (4): ReLU(inplace=True)\n",
       "    (5): Dropout(p=0.5, inplace=False)\n",
       "    (6): Linear(in_features=4096, out_features=1000, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "f4e7cbc1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "12.1\n",
      "1\n",
      "NVIDIA GeForce RTX 3050 6GB Laptop GPU\n"
     ]
    }
   ],
   "source": [
    "print(torch.cuda.is_available())\n",
    "print(torch.version.cuda)\n",
    "print(torch.cuda.device_count())\n",
    "print(torch.cuda.get_device_name(0) if torch.cuda.is_available() else \"No GPU\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "7c6459bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.0001\n",
    "epochs = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "cc191a5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, data_loader, criterion, device):\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for images, labels in data_loader:\n",
    "            images, labels = images.to(device), labels.to(device).float()\n",
    "            outputs = model(images).squeeze(1)\n",
    "            probs = torch.sigmoid(outputs)\n",
    "            predicted = (probs > 0.5).float()\n",
    "            \n",
    "            loss = criterion(outputs, labels)\n",
    "            val_loss += loss.item()\n",
    "            \n",
    "            total += labels.size(0)\n",
    "            correct += (predicted.squeeze() == labels).sum().item()\n",
    "    \n",
    "    avg_loss = val_loss / len(data_loader)\n",
    "    accuracy = 100 * correct / total\n",
    "    \n",
    "    return avg_loss, accuracy            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "d4ddf126",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, train_loader, criterion, optimizer, device, epochs, val_loader=None, validation=False):\n",
    "    history = {\n",
    "        \"training_loss\": [],\n",
    "        \"validation_loss\": [],\n",
    "        \"validation_accuracy\": []\n",
    "    }\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        for images, labels in train_loader:\n",
    "            images, labels = images.to(device), labels.to(device).float()\n",
    "            optimizer.zero_grad()\n",
    "            output = model(images).squeeze(1)\n",
    "            loss = criterion(output, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item()\n",
    "            \n",
    "        epoch_loss = running_loss / len(train_loader)    \n",
    "        history['training_loss'].append(epoch_loss)        \n",
    "        \n",
    "        if validation is True:\n",
    "            val_loss, val_acc = evaluate_model(model, val_loader, criterion, device)\n",
    "        \n",
    "            # save history\n",
    "            history['validation_loss'].append(val_loss)\n",
    "            history['validation_accuracy'].append(val_acc)\n",
    "            \n",
    "            print(f\"Epoch [{epoch+1}/{epochs}] \"\n",
    "                f\"Train Loss: {epoch_loss:.4f} | \"\n",
    "                f\"Val Loss: {val_loss:.4f} | \"\n",
    "                f\"Val Acc: {val_acc:.2f}%\")\n",
    "        \n",
    "    return  history         "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "ad41a24e",
   "metadata": {},
   "outputs": [],
   "source": [
    "for param in model.features.parameters():\n",
    "    param.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "3d3a7eeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.classifier = nn.Sequential(\n",
    "    nn.Linear(25088,128),\n",
    "    nn.ReLU(),\n",
    "    nn.Dropout(0.5),\n",
    "    \n",
    "    nn.Linear(128,64),\n",
    "    nn.ReLU(),\n",
    "    nn.Dropout(0.3),\n",
    "    \n",
    "    nn.Linear(64,32),\n",
    "    nn.ReLU(),\n",
    "    nn.Dropout(0.2),\n",
    "    \n",
    "    nn.Linear(32,1)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "e3a29d19",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.BCEWithLogitsLoss()\n",
    "optimizer = torch.optim.Adam(model.classifier.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "27385b9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "d3317748",
   "metadata": {},
   "outputs": [],
   "source": [
    "history = train_model(model, train_loader, criterion, optimizer, device, epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "5c8ace29",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'training_loss': [0.16132791357419168,\n",
       "  0.03451355179053809,\n",
       "  0.021365954313404947,\n",
       "  0.012945852592409834,\n",
       "  0.006760277770794404,\n",
       "  0.004584479250380931,\n",
       "  0.0030352861725755267,\n",
       "  0.0015535389791581106,\n",
       "  0.003178097814707002,\n",
       "  0.0009841509579663727],\n",
       " 'validation_loss': [],\n",
       " 'validation_accuracy': []}"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "dc187362",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss, accuracy = evaluate_model(model, test_loader, criterion, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "a0e498d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.07526436547935589\n",
      "Accuracy: 98.86307464162135\n"
     ]
    }
   ],
   "source": [
    "print(f\"Loss: {loss}\")\n",
    "print(f\"Accuracy: {accuracy}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
